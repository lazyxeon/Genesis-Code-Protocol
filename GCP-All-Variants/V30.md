Code Genesis Protocol (GCP) v30: Alloy Derivation Protocol with Critique Refinement and Rigorous Analysis Amendment 
## Introduction and Overview 
The Code Genesis Protocol (GCP) v30 is a structured, iterative framework designed to facilitate the invention of novel algorithms and code solutions by leveraging the alloying of existing techniques with new mathematical derivations. This version builds on v29 by adding a self-check for output integrity to prevent gremlins (e.g., embedded thoughts or cut-offs) and minor clarifications for consistency. 
The protocol remains adaptive and self-reflective, ensuring that outputs are practically functional, mathematically rigorous, verifiable through simulations and benchmarks, and superior to existing methods in key metrics such as complexity, efficiency, accuracy, or other problem-specific measures. It incorporates tools for gap assessment, mathematical derivation, code synthesis, analysis, refinement based on critiques and simulations, validation with fixes, and final benchmarking with a complete, ready-to-use Google Colab notebook. The notebook generation ensures that the invention or enhancement is packaged in a shareable, executable format that includes setup, code tests, benchmarks, and results, making it immediately usable for verification and further experimentation. 
The protocol supports problems where solutions exist but have limitations, encouraging hybrids or enhancements that are "different designs" or polished improvements with measurable benefits. It avoids placeholders, truncation, or summaries—all sections are fully expanded with detailed explanations, methods, and examples. The process is organic, with each phase building logically on the previous one, and includes explicit instructions for handling iterations, pivots, and tool fallbacks. 
## Core Principles 
- **Alloy and Enhancement Prioritization**: Outputs can be new designs via alloying or genuine enhancements of undervalued or non-SOTA methods, focusing on practical improvements rather than forced novelty. Hybrids/polishing are encouraged to create diverse or refined solutions that address gaps without unnecessary complexity. 
- **Rigorous Derivation**: All outputs must stem from first-principles math, solved step-by-step for closed-ended verification, ensuring soundness and not arbitrary. For enhancements, derive why the polish improves metrics. 
- **Iterative Refinement**: Built-in loops for analysis, critique incorporation, simulation with edge cases, and validation to ensure functionality, with explicit options to pivot if confining (e.g., switch to undervalued base). 
- **Tool Integration**: Utilize web_search for gaps and trends (broadened to include lesser-known sources), code_execution for testing and simulations, sympy for mathematical solving, and other tools as needed. Specific guidance: If sympy fails to solve an equation (e.g., due to non-linear or unsolvable form), fallback to numerical methods with numpy or scipy (e.g., use scipy.optimize.minimize to find numerical values instead of symbolic, and document "Sympy
couldn't solve exactly, approximated with scipy.optimize.minimize for min lambda = (MSE + variance)"). 
- **Novelty, Enhancement, and Usefulness**: Prioritize utility; if novel, label as such; if enhancement of non-SOTA, label "not novel but genuine enhancement" without novelty. Ensure diverse/grounded (filter pseudoscience via reliable sources) and useful (better in key metrics). - **Benchmark Notebook Generation**: During benchmarking, automatically develop a full Google Colab-ready notebook (.ipynb JSON format) with all code, tests, benchmarks, edge cases, and outputs for copy-paste execution. 
- **No Placeholders or Truncation**: All sections are fully fleshed out with explanations, examples, and no summaries—everything is explicit and ready for immediate use. - **Organic Expansion**: The protocol is described in a natural, detailed manner, with each phase expanded to include full reasoning, methods, and applications, ensuring it can be applied directly without additional interpretation. 
- **Pivot Triggers**: In addition to diversity scores, include performance triggers (e.g., if improvement <15% over SOTA in metrics like time or accuracy), complexity triggers (e.g., if time complexity worsens from SOTA, such as from O(n log n) to O(n^2)), semantic degradation triggers (e.g., if fidelity metrics show <80% preservation of meaning in domain-specific tests like NLP, pivot to a different alloy), and confinement triggers (e.g., if SOTA dominates results, pivot to undervalued sources). 
- **Semantic Fidelity Requirement**: For problems involving data with inherent meaning (e.g., tokens, text, images), Phases 4-6 must include domain-specific metrics (e.g., BLEU/ROUGE for text, PSNR for images) and checks for "meaningless" outputs (e.g., if regen produces repetitive dummies, flag as naive and pivot). 
- **Code Presentation**: All code outputs must be enclosed in <details><summary>[Appropriate Title, e.g., "View Synthesized Code"]</summary> ```python\n[code]\n``` </details> to create collapsible sections, reducing visual clutter while maintaining accessibility. This applies ONLY to pure code snippets (e.g., synthesized code, sympy executions, fixed code in loops); all non-code elements (e.g., phase summaries, reports, explanations, metrics, iteration logs) remain outside dropdowns for full visibility. Notebook JSON in Phase 7 is wrapped as a whole since it's primarily code/structure, but its metadata sections are designed to be readable when expanded. 
- **Phase Summaries**: Each phase ends with a 1-2 sentence recap for quick scanning. Lite Mode: Optional fast-track for low-complexity problems, skipping detailed derivation/analysis. - **Pseudoscience Enforcement**: Mandatory grounding/citation for claims. Scalable Mode: Guidance for cloud ports in sims. Radical Twist: Optional analogies from unrelated domains for creativity. 
- **NEW in v30**: Output Integrity Check: At end of protocol execution, self-verify no embedded thoughts/gremlins in final text (e.g., no meta-commentary outside designated sections). 
## Phases of the Protocol 
The GCP v30 consists of 7 phases, executed sequentially with loops for iteration where needed. Each phase is described in full detail below, including the purpose, inputs, methods, outputs, and examples of how it would be applied to a hypothetical problem like "Invent a new efficient sorting algorithm for large datasets with duplicates." **Lite Mode Option**: If problem flagged
"low complexity" in Phase 1, skip Phases 2/4, jump to synthesis/validation with enhancement focus. 
### Phase 0: Source Vetting (Pre-Gap) 
**Purpose**: This optional but recommended pre-phase vets search sources to enforce pseudoscience filtering, ensuring all inputs are from reliable, verifiable origins before gap assessment. This prevents ungrounded claims early, like validating materials before alloying. 
**Inputs**: User's problem statement. 
**Methods**: 
- Define reliable criteria: High (arXiv, IEEE, GitHub stars>100), Medium (reputable blogs/forums like StackOverflow), Low (unverified sites)—discard Low. 
- If search in Phase 1 yields Low, rerun with filters (e.g., "site:arxiv.org"). 
- Document vetting (e.g., "10 results: 8 High, 2 Medium—proceed"). 
**Outputs**: Vetted source list or flag "Insufficient reliable sources—pivot problem." **Example Application**: For sorting, vet MergeSort papers (High from IEEE)—proceed. **Phase Summary**: Vetted sources ensure grounded start. 
### Phase 1: Gap Assessment 
**Purpose**: This phase identifies the specific inconsistencies or limitations in existing solutions to the problem, using web_search to gather 2025 trends, SOTA, and undervalued/lesser-known methods. It formulates the core gap to fill, ensuring the output targets a real need, which could be filled by novelty or enhancement. This sets the foundation for alloying or polishing, like identifying why an older bridge design could be refined to outperform modern ones in specific contexts. By listing both SOTA and undervalued components, it prepares for hybrids or enhancements in the next phase, promoting diverse or practical designs. 
**Inputs**: The user's problem statement (e.g., "Design a novel real-time data stream anomaly detection algorithm that can adapt to changing baseline patterns without prior training data"). 
**Methods**: 
- Craft a detailed web_search query based on the problem, incorporating keywords for gaps, limitations, improvements, SOTA, and undervalued methods (e.g., "image noise reduction algorithms 2025 gaps OR limitations OR improvements OR underrated open-source 'adaptive seasonal patterns' site:arxiv.org OR site:ieee.org OR site:github.com OR filetype:pdf"). - Request 10-20 results to ensure comprehensive coverage of recent trends, academic papers, technical reports, and lesser-known sources from reliable platforms (use Phase 0 vetting to filter unreliable). 
- Analyze the results by reading titles, abstracts, and key sections to extract common limitations (e.g., "ARIMA requires prior data for baseline calibration"), gaps (e.g., "No method adapts to
changing patterns without training"), SOTA components (e.g., "ARIMA for autoregression"), and undervalued methods (e.g., "Older Kalman filters efficient but overlooked for seasonality"). - Formulate the inconsistency as a clear, concise statement that highlights the limitation and opportunity for alloying or enhancement (e.g., "SOTA like ARIMA requires prior data for baseline, failing on changing patterns without training; undervalued Kalman could be polished for better adaptivity"). 
- If no significant gap is found (e.g., the problem is already solved optimally), check for enhancement opportunities in undervalued methods; if none, pivot to a related problem (e.g., "Shift to anomaly detection in non-stationary video streams") or conclude no output is necessary, documenting reasoning (e.g., "Optimally solved by SOTA/undervalued X with no gap/enhancement, pivoting to Y"). 
- Compile a list of 2-3 components (SOTA or undervalued) that can be alloyed/polished in Phase 2, including brief descriptions of their strengths and weaknesses (e.g., "ARIMA: Strength in trend modeling, weakness in non-stationary data; Kalman (undervalued): Strength in filtering, weakness in unpolished seasonality"). 
- Cross-check for semantic gaps if applicable (e.g., if problem involves meaning-preserving tasks, note if methods lose semantics). 
- Flag complexity level for Lite Mode (e.g., "Low if enhancement-only"). 
**Outputs**: A formulated gap statement and a list of SOTA/undervalued components/trends to alloy or enhance, with descriptions (e.g., "Gap: Non-stationary streams need adaptive baselines without training; Components: ARIMA for trends (strength: autoregression, weakness: static); Kalman (undervalued) for filtering (strength: efficiency, weakness: needs polish)"). 
**Example Application**: For the hypothetical "Invent a new efficient sorting algorithm for large datasets with duplicates," the web_search might return SOTA MergeSort and undervalued older radix sorts efficient on duplicates but overlooked. The gap statement could be "Gap: Existing algorithms like MergeSort suboptimal for duplicates; undervalued radix could be polished to outperform in memory; Components: MergeSort (strength: stable O(n log n), weakness: no dup opt); Radix (undervalued, strength: O(n) on keys, weakness: needs dup handling polish)." If no gap/enhancement, pivot to "Invent sorting for non-integer with dups" and document "No opportunity in integers; pivoting for novelty/enhancement in non-integers." 
**Phase Summary**: Identified gap in duplicates handling, with undervalued radix for potential enhancement. 
### Phase 2: Alloy Derivation or Enhancement Polishing 
**Purpose**: This phase either alloys components (SOTA/undervalued) with new mathematical twists to fill the gap or polishes an undervalued method for genuine improvement, using sympy to solve equations step by step for closed-ended verification. It ensures the output is a hybrid "alloy" for novelty or a refined "enhancement" for utility, like polishing an old steel alloy to outperform modern ones in cost/efficiency. The derivation/polishing is explained in detail to show how the solution is arrived at from the gap, with explicit steps to promote diversity or practicality and avoid confinement.
**Inputs**: Gap statement and component list with descriptions from Phase 1 (e.g., "Gap: Non-stationary streams need adaptive baseline without training; Components: ARIMA for trends (strength: autoregression, weakness: static), Kalman (undervalued) for filtering (strength: efficiency, weakness: needs polish)"). 
**Methods**: 
- Decide mode: If novel gap, alloy; if enhancement opportunity in undervalued, polish (e.g., add math twist to Kalman for seasonality). Label output as "novel alloy" or "genuine enhancement." - For alloy: Select 2-3 components, formulate hybrid equation (e.g., hybrid_filter = alpha * ARIMA_trend + beta * Kalman_state + gamma * new DE term). 
- For enhancement: Focus on one undervalued, derive polish math (e.g., extend Kalman with seasonal DE dp/dt = -a p + b data_shift). 
- Use sympy to derive/solve (e.g., solve for alpha/beta minimizing loss). Fallback to numerical if fails, document. If no tool, manual algebraic approximation (e.g., "By hand: alpha ≈ 0.5 for balance"). 
- Explain fully (e.g., "Polish Kalman by adding DE for adaptivity: eq dp/dt = -a p + b shift, solved p = b shift / a; enhances non-stationary handling"). 
- Calculate diversity/utility score: For alloy D=1 - similar/total; for enhancement U=improvement % estimate. If D/U low, pivot. 
- Justification: Link to gap, prove no pseudoscience (e.g., "Based on established filter theory, cited [source], no unverified claims"). 
- For semantic, include preservation term. 
- If confining, activate "Radical Twist": Web_search for analogies ("bio-inspired [problem] solutions" or "quantum [problem] methods"), integrate twist (e.g., "Neuron-like DE for memory"). - Wrap any code (e.g., sympy snippets) in <details><summary>View Sympy Code</summary> ```python\n[code]\n``` </details>. 
**Outputs**: Alloy/enhancement equation(s) with full sympy code executed and its output (wrapped in details/summary if code), step-by-step explanation of the derivation process, score/pivot notes, final equation, labeled "novel alloy" or "genuine enhancement." 
**Example Application**: For sorting, polish radix for dups: Equation M = k*n (k radix passes) - d (skipped), sympy minimize. Explanation "Polish radix by dup-skip DE for passes." If enhancement, label "genuine enhancement." D/U=0.7, no pivot. If confining, radical twist: "Bio-sorting analogy from ant colonies for distributed dup handling." 
**Phase Summary**: Polished radix with DE, labeled genuine enhancement for efficiency gain. 
### Phase 3: Code Synthesis 
**Purpose**: This phase synthesizes the derived alloy or enhancement into functional code, ensuring it's executable and directly implements the math without placeholders. The code is structured as a class or function with a test example to verify basic running.
**Inputs**: Alloy/enhancement equation and derivation from Phase 2 (e.g., hybrid_loss = alpha * (mse + variance), with alpha solved via sympy as 0.5). 
**Methods**: 
- Synthesize code as a class/function implementing the equation (e.g., class HybridAnomalyDetector with def detect(self, data): loss = self.alpha * (arima_error(data) + fourier_error(data)); if loss > threshold: return anomaly). 
- Include all imports (e.g., import sympy as sp or from scipy.optimize import minimize for fallbacks). 
- Add a test with dummy data (e.g., data = [1,2,3+noise]; print(detect(data)) to show output). - Ensure the code is self-contained, with all necessary imports at the top, full class or function definitions, and no external non-standard packages without justification (if a fallback to numerical methods is needed from Phase 2, use scipy.optimize, and note "Fallback to numerical min as sympy couldn't solve exactly"). 
- If the derivation involved multiple variants for diversity, synthesize the primary one but note the alternatives in comments for potential future use (e.g., "Alternative alloy: Wavelet version could replace Fourier for non-periodic: coeffs = pywt.dwt(data, 'db1')"). 
- The code should be ready for immediate copy-paste and running in a Python environment, with no placeholders (e.g., if a function needs a parameter from the derivation, define it with the derived value like lambda_ = mse / perceptual, and compute it dynamically in the code). - For semantic problems, add basic meaning-check in test (e.g., assert semantic_sim(input, output) >0.8, using cosine or similar). 
- Wrap the full code snippet in <details><summary>View Synthesized Code</summary> ```python\n[code]\n``` </details>. 
**Outputs**: Full code snippet (wrapped in details/summary), including any necessary imports, the main class or function implementing the invention, and a test example with dummy data showing it runs (e.g., a short script at the end that runs the function and prints output). 
**Example Application**: For the hypothetical "Invent a new efficient sorting algorithm for large datasets with duplicates," the code would implement a HybridDupSort function that counts frequencies with CountingSort to skip comparisons in MergeSort. The full code (wrapped): <details><summary>View Synthesized Code</summary> ```python\nfrom typing import List\ndef hybrid_dup_sort(arr: List[int]) -> List[int]:\n # frequency code...\narr = [3,1,1,2,2]\nprint(hybrid_dup_sort(arr)) # [1,1,2,2,3]\n``` </details>. 
**Phase Summary**: Synthesized hybrid sort with dup skip, tested on dummy data. 
### Phase 4/5: Integrated Analysis 
**Purpose**: This merged phase performs a deep mathematical, coding, and novelty analysis, including simulations with diverse and edge-case data, to catch flaws early and ensure the invention is robust. It measures key metrics and pivots if the design is weak or confined, allowing for a change in novelty direction if desired (e.g., if the analysis shows the design is not
diverse or effective, pivot to a different alloy base like wavelet instead of Fourier, and rerun from Phase 2). Merging reduces overhead while maintaining depth. 
**Inputs**: Full code snippet from Phase 3 (the synthesized code with test example). 
**Methods**: 
- **Mathematical Analysis**: Prove or bound key properties of the invention using mathematical notation and explanations (e.g., prove time complexity O(n log n) by breaking down the algorithm steps: frequency count O(n), unique sort O(u log u), expansion O(n), total O(n + u log u) ≤ O(n log n) since u ≤ n, with proof "Since u ≤ n, u log u ≤ n log n, thus upper bound O(n log n); for lower bound on dup-heavy, when u= O(1), O(n)"). If applicable, use sympy for further solving (e.g., solve for worst-case bounds by setting u=n for max time, or u=1 for min time, and simplify the expression with sympy code like "sp.simplify(M.subs(u, n)) = n log n"). Wrap any sympy code in details/summary. 
- **Code and Novelty Review**: Conduct a static review of the code for bugs (e.g., check if all variables are defined, if loops handle zero-length inputs without infinite loops, if types match in operations like array concatenation), efficiency (e.g., calculate time/space complexity by analyzing loops and data structures, such as O(n log n) for sorting with n elements, noting if it's better than SOTA in dup cases), and conceptual soundness (e.g., verify if the code faithfully implements the derived math without deviations, such as ensuring the lambda calculation matches the sympy-solved formula exactly, or if approximations are used, justify with numerical accuracy like "Approximation error <0.01 in sim"). Assess novelty by cross-checking with the gap assessment (e.g., "Is this different from SOTA or too similar? Search shows no exact hybrid"). Calculate a simple novelty score N = 1 - (similar_components / total_components) where similar_components are those directly from SOTA without twist (e.g., if MergeSort used unchanged, similar=1 with dup twist, similar=0). 
- **Simulations**: Run the code with varied data sets, including edge cases to test robustness (e.g., empty input to check for errors or correct handling like returning empty output; large input with n=1000 elements to check scalability and time; noisy or random data to check if the algorithm handles variability well; worst-case data like all duplicates for sorting to check if the optimization triggers correctly). Use code_execution tool if needed for verification, but simulate in reasoning if tool limits apply (e.g., for large n, estimate time based on complexity and small-scale tests). For each simulation, record the inputs (e.g., "Input: arr = []"), outputs (e.g., "Output: []"), and performance (e.g., "Time: 0.001s, Memory: 100KB"). 
- **Metrics Measurement**: Compute relevant metrics for the invention (e.g., compression ratio as len(original)/len(compressed) for compression, accuracy as accuracy_score for classifiers, time as measured seconds for algorithms) and compare to SOTA baseline (e.g., standard K-means time vs. hybrid time on the same data, with code for the baseline included in the simulation to run side-by-side). Use numerical values and describe any plots or tables (e.g., "Simulation on n=100 to 1000 shows time O(n log n) curve vs. SOTA O(n^2), with table: n=100 time=0.01s invention vs 0.1s SOTA"). 
- For semantic domains, add meaning-preservation sims (e.g., compute semantic similarity via cosine on embeddings; if <80%, flag "Semantic degradation" and pivot). Cross-check outputs for "naive" patterns (e.g., repetitive dummies → "Naive regen detected, pivot required").
- Add pseudoscience scan: Flag unproven claims (e.g., "No citation for H as semantic—pivot or cite"). 
- Add semantic review if applicable (e.g., "Does output preserve meaning? Check for naive patterns like repeated averages → flag 'Naive impl: Pivot for better regen'"). Require tie-back to gap (e.g., "Does DE term directly address non-stationary? Yes/no with justification"). - If issues (e.g., poor metrics <15% better, or semantic loss), loop back to Phase 3 for fixes or Phase 2 for pivot. If confining, activate radical twist: Web_search for analogies ("bio-inspired [problem] solutions" or "quantum [problem] methods"), integrate. 
**Outputs**: Integrated report with math bounds, code/novelty review, simulation results/tables, metrics comparisons (e.g., "Time: 40% reduction"), pseudoscience flags, and any loop/pivot notes. 
**Example Application**: For the sorting hypothesis, math bounds O(n log n), code checks no index errors, sims: empty [] → [], all dup → O(n), large n=10^4 → 0.1s vs SOTA 0.2s. Metrics: Time reduction 40%, N=0.67. Pseudoscience: None. Semantic N/A, no pivot. 
**Phase Summary**: Analysis passed with 40% time gain, no issues. 
### Phase 6: Validation/Fix Loop 
**Purpose**: Use code_execution tool to test the code with real data, fix any runtime errors or performance issues until it works and is better than SOTA in key metrics. This phase ensures the invention is executable and meets practical standards. 
**Inputs**: Code from Phase 4/5 (the reviewed and potentially fixed code). 
**Methods**: 
- Prepare real or generated data relevant to the problem (e.g., for compression, a sample PDF bytes string with repetitive sections like b"%PDF-1.4\n<<obj>> 50\n<< /obj >>"). - Run the code using code_execution tool, capturing outputs, errors, and performance (e.g., time taken, memory used). 
- If errors occur (e.g., NameError for missing import, TypeError for mismatch), fix them (e.g., add the import, adjust types to match like convert to bytes), and rerun the test with the fixed code. Wrap fixes in details/summary. 
- Check for correctness in the output (e.g., for compression, verify decompressed == original with print("Extractable:", decompressed == original); for sorting, verify the list is sorted correctly with sorted(arr) == hybrid_dup_sort(arr)). 
- Measure key metrics and compare to SOTA baseline (e.g., for compression, compute ratio = len(original)/len(compressed) and compare to zlib.compress on the same data with zlib_ratio = len(original)/len(zlib.compress(original))). 
- For semantic tasks, add domain-specific validation (e.g., if text tokens, compute BLEU(decompressed, original)>0.8; if <, fix by improving regen logic or pivot).
- Loop until no errors, the invention works correctly (e.g., extractable=True for compression), and metrics are better than SOTA (e.g., ratio 1.2x zlib, or time < SOTA by 10%, or semantic fidelity >80%). Limit to max 3 iterations. 
- If unfixable or metrics not better (e.g., ratio < SOTA after 3 fixes), pivot the novelty (e.g., change from phi to wavelet for better efficiency) and rerun from Phase 2, documenting the pivot (e.g., "Pivot: Ratio only 1.1x zlib—switching to wavelet alloy for better pattern detection, rerunning Phase 2"). 
- Document each iteration in the loop (e.g., "Iteration 1: Error: Missing import zlib—fixed by adding import zlib. Ratio: 1.5 > zlib 1.3—success"). 
**Outputs**: Fixed code (wrapped in details/summary if changed), test outputs showing success (e.g., "Ratio: 2.1, Extractable: True"), and iteration log if fixes were needed (e.g., "Iteration 1: Fixed missing import, ratio improved to 1.5"). 
**Example Application**: For the sorting hypothesis, prepare data like arr = [1]*100, run code_execution: No error, sorted correctly, time 10% < Timsort on dups. Semantic N/A, no pivot. 
**Phase Summary**: Validated sort with fixes, 10% time improvement. 
### Phase 7: Benchmarking and Notebook Generation 
**Purpose**: Run comprehensive benchmarks comparing the invention to SOTA baselines, generate a full .ipynb notebook with all code, tests, and results, and finalize the invention if metrics confirm superiority. 
**Inputs**: Validated code from Phase 6. 
**Methods**: 
- Implement SOTA baseline in code (e.g., import zlib for compression comparison). - Run benchmarks on multiple datasets (e.g., small/medium/large, varied types), measuring metrics (e.g., timeit for time, fidelity for preservation). For large-scale, add Scalable Mode: Guidance for cloud (e.g., "Adapt to Colab/GCP: !pip install dask; use dask.array for parallel sims"). Fallback to subsampling if no cloud. 
- Generate .ipynb JSON: Sections for intro (gap/alloy), code, tests (edge cases with prints), benchmarks (timeit, plots with matplotlib), results (prints/tables, improvement %). - For semantic tasks, mandate semantic benchmarks (e.g., ROUGE score > SOTA); ensure notebook checks for naive outputs (e.g., assert not all(regen == mean)). 
- If metrics not better/novel/useful, loop to Phase 2. 
- Wrap the entire .ipynb JSON in <details><summary>View Full Notebook JSON</summary> [json here] </details> for presentation. 
**Outputs**: Benchmark report with comparisons (e.g., "Time: 0.5s vs. SOTA 1.0s. Novelty: No match. Useful: Yes"), and full .ipynb JSON (wrapped in details/summary).
**Example Application**: For sorting, report "Time reduction 40% vs Timsort on 50% dups". Notebook (wrapped): <details><summary>View Full Notebook JSON</summary> { "cells": [ ... ] } </details>. 
**Phase Summary**: Benchmarked sort with 40% gain, notebook generated. 
This GCP v30 is now complete with the amendment for precise code-only wrapping, ensuring metadata like phase summaries remain outside dropdowns for better readability. If you have a specific problem to run through v30, let me know!
